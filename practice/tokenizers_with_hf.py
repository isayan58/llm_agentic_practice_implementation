# -*- coding: utf-8 -*-
"""Tokenizers_With_HF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10BDiC0YABIZhiOkgdLiw0ZHgtT-15VE8
"""

!pip install -q --upgrade datasets==3.6.0

!pip install -U "transformers==4.57.6" "torch" "accelerate" "sentencepiece"

!pip install -U transformers

# from transformers import pipeline
# import torch
# from google.colab import userdata
# from diffusers import DiffusionPipeline
# from datasets import load_dataset
# import soundfile as sf
# from IPython.display import Audio
# from __future__ import annotations
from huggingface_hub import login as hf_login
from google.colab import userdata
from huggingface_hub import login
from transformers import AutoTokenizer

def login(token, add_to_git_credential):
    """
    Log in to Hugging Face using a personal access token.

    Args:
        token: Hugging Face token string (e.g., "hf_...").
        add_to_git_credential: If True, also store the token in git credential helper.
    """
    if not token:
        raise ValueError("Hugging Face token is required.")
    if not token.startswith("hf_"):
        raise ValueError("Hugging Face token must start with 'hf_'.")
    # This will validate and persist the token in the HF cache.
    hf_login(token=token, add_to_git_credential=add_to_git_credential)

hf_token = userdata.get("HF_TOKEN")
if hf_token and hf_token.startswith("hf_"):
  print("HuggingFace key looks good so far.")
else:
  print("HuggingFace key is not set.")
login(hf_token, add_to_git_credential=True)

tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)

text = "I am excited to show Tokenizers in action to my LLM engineers"
tokens = tokenizer.encode(text)
tokens

character_count = len(text)
word_count = len(text.split(' '))
token_count = len(tokens)
print(f"There are {character_count} characters, {word_count} words and {token_count} tokens")

tokenizer.decode(tokens)

tokenizer.batch_decode(tokens)

# tokenizer.vocab
tokenizer.get_added_vocab()

len(tokenizer.vocab)

